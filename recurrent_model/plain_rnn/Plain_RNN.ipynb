{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_And_Transforms import FigrimFillersDataset, Downsampling, ToTensor, ExpandTargets, Targets2D\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([ToTensor(),Downsampling(10)])\n",
    "    \n",
    "#load split data\n",
    "figrim_dataset_train = FigrimFillersDataset(json_file='allImages_unfolded_train.json',\n",
    "                                    root_dir='figrim/fillerData/Fillers',\n",
    "                                     transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader_train = torch.utils.data.DataLoader(figrim_dataset_train, batch_size=1, \n",
    "                                             shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(dataset_loader_train): #start at index 0\n",
    "            # get the inputs\n",
    "            data = example[\"image\"]\n",
    "            #print(\"data size: {}\".format(data.size()))\n",
    "            target = example[\"fixations\"]\n",
    "            if i == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_2d_to_index_fl(tensor_size, index_2d):\n",
    "    \"\"\"\n",
    "    Takes 2D-Tensor-Size and the index (as a tensor) of one if its entries.\n",
    "    Returns the index of this entry for the flattened version of the 2D-Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_cols = tensor_size[-1]\n",
    "    idx_row, idx_col = tuple(index_2d)\n",
    "    #extract values from tensors\n",
    "    idx_row = idx_row.item()\n",
    "    idx_col = idx_col.item()\n",
    "    \n",
    "    return idx_row * n_cols + idx_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10002])\n",
      "torch.Size([7, 1, 10002])\n"
     ]
    }
   ],
   "source": [
    "new_target = torch.zeros(target.size(1), 10002)\n",
    "tensor = torch.zeros(7, 1, 10002)\n",
    "print(new_target.size())\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_inputs(data, target):\n",
    "    #TIME-STEP-INPUTS: SOS and Fixations\n",
    "\n",
    "    n_classes = data.size(-2) * data.size(-1) + 2 #plus sos- and eos-token; so here 10002\n",
    "\n",
    "    #batch-dimension (batch-size is always one), as needed to use nn.NLLLoss()\n",
    "    inputs = torch.zeros(target.size(1)+1, 1, n_classes) #0 is batch-dimension, 1 is number of fixations, 2 is fixations\n",
    "\n",
    "    #start-of-sequence-token\n",
    "    inputs[0,0,0] = 1\n",
    "\n",
    "    #fixations\n",
    "    idx_new_targets = []\n",
    "    for i in range(target.size(1)):\n",
    "        idx_new_target = index_2d_to_index_fl(data.size(), target[0,i])\n",
    "        idx_new_targets.append(idx_new_target)\n",
    "        inputs[i+1, 0, idx_new_target+1] = 1\n",
    "    return (inputs, idx_new_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_targets(data, target, idx_new_targets):\n",
    "    #TARGETS: Fixations and EOS\n",
    "    #AS WE USE nn.NLLLOSS(), only the INDEX of the target at each time step is needed, not a whole one-hot-vector\n",
    "\n",
    "    n_classes = data.size(-2) * data.size(-1) + 2 #plus sos- and eos-token; so here 10001\n",
    "\n",
    "    #index of eos-token\n",
    "    idx_new_targets.append(n_classes - 1)\n",
    "\n",
    "    #list2tensor\n",
    "    new_targets = torch.LongTensor(idx_new_targets)\n",
    "    return new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image(data):\n",
    "    #IMAGE\n",
    "\n",
    "    #batch-dimension (batch-size is always one), as needed to use nn.NLLLoss()\n",
    "    return data.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(image_size + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(image_size + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, image, input, hidden):\n",
    "        input_combined = torch.cat((image, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 30000#data.size(-3) * data.size(-2) * data.size(-1)\n",
    "n_classes = 10002\n",
    "input_size = n_classes\n",
    "hidden_size = n_classes\n",
    "output_size = n_classes\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "#output, loss = train(data, inputs, new_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.00005\n",
    "\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(image, inputs, targets):\n",
    "    targets.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(inputs.size(0)):\n",
    "        output, hidden = rnn(image, inputs[i], hidden)\n",
    "        l = criterion(output, targets[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    #for p in rnn.parameters():\n",
    "    #    p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / inputs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 100, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 12s (0 0%) 133.1399\n",
      "0m 21s (1 25%) 12103.2083\n",
      "0m 33s (2 50%) 89234688.0000\n",
      "0m 44s (3 75%) nan\n",
      "0m 54s (4 100%) nan\n"
     ]
    }
   ],
   "source": [
    "n_iters = 4\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, example in enumerate(dataset_loader_train): #start at index 0\n",
    "    # get the inputs\n",
    "    data = example[\"image\"]\n",
    "    #print(\"data size: {}\".format(data.size()))\n",
    "    target = example[\"fixations\"]\n",
    "    \n",
    "    inputs, idx_new_targets = mk_inputs(data, target)\n",
    "    new_targets = mk_targets(data, target, idx_new_targets)\n",
    "    image = flatten_image(data)\n",
    "    \n",
    "    output, loss = train(image, inputs, new_targets)\n",
    "    total_loss += loss\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), i, i / n_iters * 100, loss))\n",
    "\n",
    "    if i % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0\n",
    "        \n",
    "        \n",
    "    if i == n_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4950])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_targets.size())\n",
    "new_targets.unsqueeze(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
