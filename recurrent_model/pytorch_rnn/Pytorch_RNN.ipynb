{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.076809883117676\n",
      "9.181554794311523\n",
      "10.102762222290039\n",
      "9.249476432800293\n",
      "9.239387512207031\n",
      "8.802957534790039\n",
      "9.195699691772461\n",
      "9.25375747680664\n",
      "10.003707885742188\n",
      "9.066916465759277\n",
      "9.47988224029541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR, StepLR\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import sys\n",
    "#status bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import exactly in this way to make sure that matplotlib can generate\n",
    "#a plot without being connected to a display \n",
    "#(otherwise _tkinter.TclError: couldn't connect to display localhost:10.0)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Dataset and Transforms\n",
    "from utils.Dataset_And_Transforms import FigrimFillersDataset, Downsampling, ToTensor, SequenceModeling\n",
    "from utils.Create_Datasets import create_datasets \n",
    "    \n",
    "\n",
    "#Create datasets\n",
    "train_loader, val_loader, test_loader = create_datasets(batch_size=1, data_transform=transforms.Compose([ToTensor(), Downsampling(10), SequenceModeling()]))\n",
    "\n",
    "#Model\n",
    "#input: image and fixation-input together\n",
    "#expects input of shape (seq_len, batch, input_size), but we have (batch, seq_len, input_size), so arg batch_first True\n",
    "rnn = nn.RNN(input_size = 40002, hidden_size=10002, num_layers=1, batch_first=True)\n",
    "\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i, example in enumerate(train_loader): #start at index 0\n",
    "    # get the inputs\n",
    "    image = example[\"image\"]\n",
    "    inputs = example[\"inputs\"]\n",
    "    targets = example[\"targets\"]\n",
    "    inputs_combined = example[\"inputs_combined\"]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #hidden = torch.zeros(1,1,10002)\n",
    "    \n",
    "    output, hidden = rnn(inputs_combined)\n",
    "    #CrossEntropyLoss with RNN: output should be \n",
    "    #[1, number_of_classes, seq_length], while your target should be [1, seq_length].\n",
    "    #https://discuss.pytorch.org/t/pytorch-lstm-target-dimension-in-calculating-cross-entropy-loss/30398\n",
    "    #so switch axes from (1,6,10002 to 1,10002,6)\n",
    "    output = output.permute(0,2,1)\n",
    "    loss = criterion(output, targets)\n",
    "    \n",
    "    #loss = 0\n",
    "    #for j in range(inputs.size(1)):\n",
    "    #    output, hidden = rnn(inputs_combined[0,j].view(1,1,-1), hidden)\n",
    "    #    loss += criterion(output.view(1,-1), targets[0,j].view(-1))\n",
    "            \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 10002])\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn(inputs_combined) #if hidden_state_0 is not specified, it is just initialized as zeros\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 10002])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 40002])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.size()\n",
    "print(inputs.size())\n",
    "image.size()\n",
    "inputs_combined.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.empty(6,40002)\n",
    "\n",
    "a = torch.randn(6,10002)\n",
    "b = torch.randn(30000)\n",
    "\n",
    "for i in range(a.size(0)):\n",
    "    c[i] = torch.cat((a[i],b),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 40002])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10002])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3648)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3556)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.randn(1,10002)\n",
    "criterion(output, torch.Tensor([5]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(1, 5, requires_grad=True)\n",
    "target = torch.empty(1, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([5]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2866,  0.3221,  1.2652, -0.2681, -0.8693,  1.1387,  0.6439]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,7)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2866,  0.3221,  1.2652, -0.2681, -0.8693,  1.1387,  0.6439])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].view(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
