{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import sys\n",
    "#status bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import exactly in this way to make sure that matplotlib can generate\n",
    "#a plot without being connected to a display \n",
    "#(otherwise _tkinter.TclError: couldn't connect to display localhost:10.0)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Dataset and Transforms\n",
    "from utils.Dataset_And_Transforms import FigrimFillersDataset, Downsampling, ToTensor, ExpandTargets, Targets2D\n",
    "#Loss-Function\n",
    "from utils.MyLoss import myLoss \n",
    "#Early stopping\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "#Adapted Sigmoid Function\n",
    "from utils.MyActivationFunctions import mySigmoid\n",
    "#Live-Plotting with Visdom\n",
    "from utils.MyVisdom import VisdomLinePlotter\n",
    "#evaluation\n",
    "from utils.Evaluate_Baseline import map_idx, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
       "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
       "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
       "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
       "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
       "        84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97.,\n",
       "        98., 99.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(128,128,100,100)\n",
    "b = torch.ones(128,1,100,100)\n",
    "c = torch.cat((a,b),1) #concatenate along channel dimension\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 100, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(1,1,100,100)\n",
    "c = b.repeat(128,1,1,1)\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3748,  0.5603, -1.4218, -0.4523, -1.3813],\n",
       "        [ 1.1592, -1.4968, -0.2917, -0.4210,  0.6145],\n",
       "        [-0.3284,  0.8345,  0.6765, -0.2781, -0.7563],\n",
       "        [ 1.0577,  0.9099, -0.1743,  0.4969,  0.6981],\n",
       "        [ 0.9722, -0.1175,  0.4325, -1.4201, -1.2895]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.load('resnet-outputs.pt')[0]\n",
    "a = a.view(a.size()[-3],a.size()[-2],a.size()[-1])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.7735e-05, 5.8678e-05, 5.9617e-05,  ..., 5.9617e-05, 5.8678e-05,\n",
       "         5.7735e-05],\n",
       "        [5.8678e-05, 5.9636e-05, 6.0590e-05,  ..., 6.0590e-05, 5.9636e-05,\n",
       "         5.8678e-05],\n",
       "        [5.9617e-05, 6.0590e-05, 6.1559e-05,  ..., 6.1559e-05, 6.0590e-05,\n",
       "         5.9617e-05],\n",
       "        ...,\n",
       "        [5.9617e-05, 6.0590e-05, 6.1559e-05,  ..., 6.1559e-05, 6.0590e-05,\n",
       "         5.9617e-05],\n",
       "        [5.8678e-05, 5.9636e-05, 6.0590e-05,  ..., 6.0590e-05, 5.9636e-05,\n",
       "         5.8678e-05],\n",
       "        [5.7735e-05, 5.8678e-05, 5.9617e-05,  ..., 5.9617e-05, 5.8678e-05,\n",
       "         5.7735e-05]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Center Bias wie bei Schuett\n",
    "a = torch.randn(100,100)\n",
    "sigmax = 55\n",
    "sigmay = 55\n",
    "gridx = torch.Tensor(range(a.size()[-2]))\n",
    "gridx = gridx - torch.mean(gridx)\n",
    "gridx = gridx ** 2\n",
    "gridx = gridx / (sigmax ** 2)\n",
    "#so that gridx + gridy will give row- and column-wise combination\n",
    "gridx = gridx.view(gridx.size()[0],1)\n",
    "\n",
    "gridy = torch.Tensor(range(a.size()[-1]))\n",
    "gridy = gridy - torch.mean(gridy)\n",
    "gridy = gridy ** 2\n",
    "gridy = gridy / (sigmay ** 2)\n",
    "\n",
    "grid = gridx + gridy\n",
    "CB = torch.exp(-0.5*grid)\n",
    "CB = CB / torch.sum(CB)\n",
    "CB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(batch_size):\n",
    "    \n",
    "    #transforms\n",
    "    data_transform = transforms.Compose([ToTensor(),Downsampling(10), Targets2D(100,100,100)])#ExpandTargets(100)])\n",
    "    \n",
    "    #load split data\n",
    "    figrim_dataset_train = FigrimFillersDataset(json_file='allImages_unfolded_train.json',\n",
    "                                        root_dir='figrim/fillerData/Fillers',\n",
    "                                         transform=data_transform)\n",
    "\n",
    "    figrim_dataset_val = FigrimFillersDataset(json_file='allImages_unfolded_val.json',\n",
    "                                        root_dir='figrim/fillerData/Fillers',\n",
    "                                         transform=data_transform)\n",
    "\n",
    "    figrim_dataset_test = FigrimFillersDataset(json_file='allImages_unfolded_test.json',\n",
    "                                        root_dir='figrim/fillerData/Fillers',\n",
    "                                         transform=data_transform)\n",
    "    \n",
    "    #create data loaders\n",
    "    #set number of threads to 8 so that 8 processes will transfer 1 batch to the gpu in parallel\n",
    "    dataset_loader_train = torch.utils.data.DataLoader(figrim_dataset_train, batch_size=batch_size, \n",
    "                                             shuffle=True, num_workers=8)\n",
    "\n",
    "    dataset_loader_val = torch.utils.data.DataLoader(figrim_dataset_val, batch_size=batch_size, \n",
    "                                                 shuffle=True, num_workers=8)\n",
    "    \n",
    "    \n",
    "    #no shuffling, as to be able to identify which images were processed well/not so well\n",
    "    dataset_loader_test = torch.utils.data.DataLoader(figrim_dataset_test, batch_size=batch_size, \n",
    "                                                 shuffle=False, num_workers=8)\n",
    "    \n",
    "    return dataset_loader_train, dataset_loader_val, dataset_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0060, 0.0610, 0.2420, 0.3830, 0.2420, 0.0610, 0.0060],\n",
       "        [0.0060, 0.0610, 0.2420, 0.3830, 0.2420, 0.0610, 0.0060],\n",
       "        [0.0060, 0.0610, 0.2420, 0.3830, 0.2420, 0.0610, 0.0060],\n",
       "        [0.0060, 0.0610, 0.2420, 0.3830, 0.2420, 0.0610, 0.0060],\n",
       "        [0.0060, 0.0610, 0.2420, 0.3830, 0.2420, 0.0610, 0.0060]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([0.006, 0.061, 0.242, 0.383, 0.242, 0.061, 0.006]).repeat(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"exp\" not implemented for 'torch.LongTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-57e8a515b418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# the product of two gaussian distributions for two different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# variables (in this case called x and y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m gaussian_kernel = (1./(2.*math.pi*variance)) *torch.exp(-torch.sum((xy_grid - mean)**2., dim=-1) /(2*variance)\n\u001b[0m\u001b[1;32m     18\u001b[0m                   )\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Make sure sum of values in gaussian kernel equals 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"exp\" not implemented for 'torch.LongTensor'"
     ]
    }
   ],
   "source": [
    "# Set these to whatever you want for your gaussian filter\n",
    "kernel_size = 15\n",
    "sigma = 3\n",
    "\n",
    "# Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "x_cord = torch.arange(kernel_size)\n",
    "x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "y_grid = x_grid.t()\n",
    "xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n",
    "\n",
    "mean = (kernel_size - 1)/2.\n",
    "variance = sigma**2.\n",
    "\n",
    "# Calculate the 2-dimensional gaussian kernel which is\n",
    "# the product of two gaussian distributions for two different\n",
    "# variables (in this case called x and y)\n",
    "gaussian_kernel = (1./(2.*math.pi*variance)) *torch.exp(-torch.sum((xy_grid - mean)**2., dim=-1) /(2*variance)\n",
    "                  )\n",
    "# Make sure sum of values in gaussian kernel equals 1.\n",
    "gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "# Reshape to 2d depthwise convolutional weight\n",
    "gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                            kernel_size=kernel_size, groups=channels, bias=False)\n",
    "\n",
    "gaussian_filter.weight.data = gaussian_kernel\n",
    "gaussian_filter.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeGaussian(size, fwhm = 3, center=None):\n",
    "    \"\"\" Make a square gaussian kernel.\n",
    "    size is the length of a side of the square\n",
    "    fwhm is full-width-half-maximum, which\n",
    "    can be thought of as an effective radius.\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.arange(0, size, 1, float)\n",
    "    y = x[:,np.newaxis]\n",
    "    print(x)\n",
    "    print(y)\n",
    "    if center is None:\n",
    "        x0 = y0 = size // 2\n",
    "    else:\n",
    "        x0 = center[0]\n",
    "        y0 = center[1]\n",
    "    \n",
    "    return np.exp(-4*np.log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4.]\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.320802967661667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(makeGaussian(5, fwhm=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type float without overflow: 1.43621e+44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1af4aeb5348e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgaussian_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-1af4aeb5348e>\u001b[0m in \u001b[0;36mgaussian_map\u001b[0;34m(activations, sigma, gpu)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgaussian_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgaussian_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type float without overflow: 1.43621e+44"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "#always with mean 0,0?\n",
    "def gaussian(x,y,sigma):\n",
    "    return 1/(2*math.pi*sigma**2) * math.exp(-(x**2+y**2)/2*sigma**2)\n",
    "\n",
    "def gaussian_map(activations, sigma, gpu):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "    - activations: activations array (last two dimensions are used, these have to be odd and square)\n",
    "    - sigma: standard deviation for 2D-Gauss-Function (mean is always (0,0)) [Tensor with one scalar]\n",
    "    - gpu: bool: Do computations on gpu or cpu \n",
    "    Gives:\n",
    "    - Array of the size of the last two dimensions with the 2D-Gauss-Function evaluated at each array entry\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = activations.size()[-2]\n",
    "    cols = activations.size()[-1]\n",
    "    gaussian_map = torch.zeros(rows, cols)\n",
    "    \n",
    "    if gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            gaussian_map = gaussian_map.to('cuda')\n",
    "    \n",
    "    mean_row = rows // 2\n",
    "    mean_col = cols // 2\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            gaussian_map[i,j] = gaussian(abs(i-mean_row),abs(j-mean_col),sigma.item())\n",
    "    return gaussian_map\n",
    "\n",
    "#square, uneven activations array\n",
    "activations = torch.randn(1,1,15,15)\n",
    "sigma = torch.Tensor([1])\n",
    "gaussian_map(activations, sigma, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self, gpu=False):\n",
    "        super(TestNet, self).__init__()\n",
    "        #3 input image channels (color-images), 64 output channels,  3x3 square convolution kernel\n",
    "        #padding to keep dimensions of output at 100x100\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 1, 3, stride=1, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(1)\n",
    "        #self.pool1 = nn.AdaptiveMaxPool2d((50,50))\n",
    "        self.conv4 = nn.Conv2d(1, 1, 1, stride=1, padding=0)\n",
    "        #pooling so that dimension is reduced by one (from 100 to 99), so that we have a middle pixel\n",
    "        #for the Gaussian mask; stride=1 so that we only reduce by 1 pixel\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.gauss_sigma = nn.Parameter(torch.Tensor([1]))\n",
    "        self.gauss_sigma.requires_grad_()\n",
    "        #self.gauss.weight = gaussian_weights\n",
    "        #scale parameter for the sigmoid function\n",
    "        self.upper_bound = nn.Parameter(torch.Tensor([1]))\n",
    "        #make it considered by autograd\n",
    "        self.upper_bound.requires_grad_()\n",
    "        self.gpu = gpu\n",
    "        if gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                self.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"input sum at beginning of forward pass: {}\".format(torch.sum(x)))\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        #print(\"input sum after first conv and relu: {}\".format(torch.sum(x)))\n",
    "        x = self.conv1_bn(x)\n",
    "        #print(\"input sum after first batch normalization: {}\".format(torch.sum(x)))\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        #print(\"input sum after second conv and relu: {}\".format(torch.sum(x)))\n",
    "        x = self.conv2_bn(x)\n",
    "        #print(\"output shape: {}\".format(x.size()))\n",
    "        #print(\"input sum after second batch normalization: {}\".format(torch.sum(x)))\n",
    "        #if scaled by a negative value, we would try to take the ln of negative values in the loss  function\n",
    "        #(ln is not defined for negative values), so make sure that the scaling parameter is positive\n",
    "        #x = mySigmoid(self.conv3(x), abs(self.upper_bound), gpu)\n",
    "        #x = functional.relu(self.conv3(x))\n",
    "        x = functional.relu(self.conv3(x))\n",
    "        x = self.conv3_bn(x)\n",
    "        #print(\"input sum after last conv and sigmoid: {}\".format(torch.sum(x)))\n",
    "        #x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool1(x)\n",
    "        x = gaussian_map(x, self.gauss_sigma, gpu) * x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "\n",
    "#initilaize the NN\n",
    "model = TestNet(gpu)\n",
    "train_loader, val_loader, test_loader = create_datasets(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0179, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#model = model.to('cpu')\n",
    "t = iter(test_loader)\n",
    "for i, example in enumerate(t): #start at index 0\n",
    "            # get the inputs\n",
    "            data = example[\"image\"]\n",
    "            #print(\"input sum: {}\".format(torch.sum(data)))\n",
    "            target = example[\"fixations\"]\n",
    "            #target_locs = example[\"fixation_locs\"]\n",
    "            \n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            \n",
    "            output = model(data)\n",
    "            if i == 0:\n",
    "                break\n",
    "print(output[0,0,50,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05854983152431917"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian(abs(0-1),abs(0-1),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9435148500492928e-141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-(1**2+1**2)/2*18**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004912189601601709"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(2*math.pi*18**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(0,5,1,float)\n",
    "y = x[:, np.newaxis]\n",
    "y\n",
    "torch.from_numpy(makeGaussian(5)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.003765\t0.015019\t0.023792\t0.015019\t0.003765\n",
    "0.015019\t0.059912\t0.094907\t0.059912\t0.015019\n",
    "0.023792\t0.094907\t0.150342\t0.094907\t0.023792\n",
    "0.015019\t0.059912\t0.094907\t0.059912\t0.015019\n",
    "0.003765\t0.015019\t0.023792\t0.015019\t0.003765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[0.0850, 0.2143, 0.2916, 0.2143, 0.0850],\n",
      "          [0.2143, 0.5400, 0.7349, 0.5400, 0.2143],\n",
      "          [0.2916, 0.7349, 1.0000, 0.7349, 0.2916],\n",
      "          [0.2143, 0.5400, 0.7349, 0.5400, 0.2143],\n",
      "          [0.0850, 0.2143, 0.2916, 0.2143, 0.0850]]]])\n"
     ]
    }
   ],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self, gpu=False):\n",
    "        super(TestNet, self).__init__()\n",
    "        #3 input image channels (color-images), 64 output channels,  3x3 square convolution kernel\n",
    "        #padding to keep dimensions of output at 100x100\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        print(self.conv1.weight.size())\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 1, 3, stride=1, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(1)\n",
    "        #self.pool1 = nn.AdaptiveMaxPool2d((50,50))\n",
    "        self.conv4 = nn.Conv2d(1, 1, 1, stride=1, padding=0)\n",
    "        self.gauss = nn.Conv2d(1, 1, 5, padding =2, bias=False)\n",
    "        #has to have size (1,1,5,5), as 1 output channel, 1 input channel, 5x5 convolution\n",
    "        self.gauss.weight = nn.Parameter(torch.from_numpy(makeGaussian(5)).view(1,1,5,5).float(), requires_grad=False)\n",
    "        print(self.gauss.weight)\n",
    "        #self.gauss.weight = gaussian_weights\n",
    "        #scale parameter for the sigmoid function\n",
    "        self.upper_bound = nn.Parameter(torch.Tensor([1]))\n",
    "        #make it considered by autograd\n",
    "        self.upper_bound.requires_grad_()\n",
    "        self.gpu = gpu\n",
    "        if gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                self.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"input sum at beginning of forward pass: {}\".format(torch.sum(x)))\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        #print(\"input sum after first conv and relu: {}\".format(torch.sum(x)))\n",
    "        x = self.conv1_bn(x)\n",
    "        #print(\"input sum after first batch normalization: {}\".format(torch.sum(x)))\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        #print(\"input sum after second conv and relu: {}\".format(torch.sum(x)))\n",
    "        x = self.conv2_bn(x)\n",
    "        #print(\"output shape: {}\".format(x.size()))\n",
    "        #print(\"input sum after second batch normalization: {}\".format(torch.sum(x)))\n",
    "        #if scaled by a negative value, we would try to take the ln of negative values in the loss  function\n",
    "        #(ln is not defined for negative values), so make sure that the scaling parameter is positive\n",
    "        #x = mySigmoid(self.conv3(x), abs(self.upper_bound), gpu)\n",
    "        #x = functional.relu(self.conv3(x))\n",
    "        x = functional.relu(self.conv3(x))\n",
    "        x = self.conv3_bn(x)\n",
    "        #print(\"input sum after last conv and sigmoid: {}\".format(torch.sum(x)))\n",
    "        #x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.gauss(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#initilaize the NN\n",
    "model = TestNet(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.0951,  -8.7581, -10.0444,  ..., -10.4240,  -9.5387,  -6.9138],\n",
      "         [ -8.5760, -12.3445, -14.0827,  ..., -14.6083, -13.2694,  -9.5698],\n",
      "         [ -9.7867, -14.0851, -15.9635,  ..., -16.5652, -14.8720, -10.6190],\n",
      "         ...,\n",
      "         [ -4.6591,  -6.3251,  -6.9735,  ...,  -6.9735,  -6.3251,  -4.6592],\n",
      "         [ -4.2749,  -5.7656,  -6.3251,  ...,  -6.3124,  -5.7336,  -4.2314],\n",
      "         [ -3.1967,  -4.2749,  -4.6591,  ...,  -4.6325,  -4.2078,  -3.1053]]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#model = model.to('cpu')\n",
    "t = iter(test_loader)\n",
    "for i, example in enumerate(t): #start at index 0\n",
    "            # get the inputs\n",
    "            data = example[\"image\"]\n",
    "            #print(\"input sum: {}\".format(torch.sum(data)))\n",
    "            target = example[\"fixations\"]\n",
    "            #target_locs = example[\"fixation_locs\"]\n",
    "            \n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            \n",
    "            output = model(data)\n",
    "            if i == 0:\n",
    "                break\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.PoissonNLLLoss(log_input=True, full=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.view(-1, target.size()[-1], target.size()[-2])\n",
    "loss = criterion(output, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MyLoss import myLoss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = myLoss2(output, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.5839e+01, -2.0886e+01, -1.2744e+01,  ..., -1.7392e+01,\n",
      "           -1.5468e+01,  1.3891e+00],\n",
      "          [-1.5319e+01, -1.2040e+01, -8.3850e+00,  ..., -6.8545e+00,\n",
      "           -1.3909e+01,  3.7658e+00],\n",
      "          [-1.9382e+01, -1.4894e+01, -6.8167e+00,  ..., -5.5862e+00,\n",
      "           -1.6090e+01,  5.4470e+00],\n",
      "          ...,\n",
      "          [-9.4274e+00, -5.4895e+00, -2.4079e-01,  ...,  1.9000e+00,\n",
      "           -6.5631e+00,  6.4394e+00],\n",
      "          [-9.0873e+00, -3.3145e+00,  1.4876e+00,  ...,  4.3158e+00,\n",
      "           -2.4395e+00,  6.7142e+00],\n",
      "          [-3.6674e+00,  2.6020e+00,  2.6126e+00,  ...,  8.2902e+00,\n",
      "            7.0707e-01,  5.9727e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.1118e+01, -1.2144e+01, -8.8363e+00,  ..., -1.0303e+01,\n",
      "           -1.0382e+01, -3.4797e-01],\n",
      "          [-1.2356e+01, -7.9673e+00, -7.1867e+00,  ..., -5.7309e+00,\n",
      "           -8.2004e+00, -1.7581e-01],\n",
      "          [-1.3366e+01, -1.1009e+01, -6.9016e+00,  ..., -5.9062e+00,\n",
      "           -8.2549e+00,  2.1992e+00],\n",
      "          ...,\n",
      "          [-1.5907e+01, -1.0436e+01, -4.0496e+00,  ..., -1.5370e+00,\n",
      "           -4.2793e+00,  5.0303e+00],\n",
      "          [-1.4047e+01, -8.1242e+00, -2.9440e+00,  ...,  4.3420e-01,\n",
      "           -2.7090e+00,  5.0064e+00],\n",
      "          [-5.2804e+00, -5.0277e-01,  9.8161e-01,  ...,  4.6330e+00,\n",
      "            1.6870e+00,  6.1499e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.8922e+00, -6.9426e+00, -6.9902e+00,  ..., -1.4496e+01,\n",
      "           -1.0428e+01,  4.9468e-01],\n",
      "          [-8.1134e+00, -6.8548e+00, -4.6804e+00,  ..., -1.0499e+01,\n",
      "           -1.0329e+01,  2.4522e+00],\n",
      "          [-1.0732e+01, -1.1285e+01, -6.3956e+00,  ..., -5.7447e+00,\n",
      "           -1.2191e+01,  3.6286e+00],\n",
      "          ...,\n",
      "          [-1.1612e+01, -1.0484e+01, -2.6654e+00,  ..., -1.3028e+00,\n",
      "           -9.7483e+00,  5.8362e+00],\n",
      "          [-1.1150e+01, -7.3661e+00, -2.1319e+00,  ...,  2.3206e+00,\n",
      "           -5.0117e+00,  4.6841e+00],\n",
      "          [-4.7381e+00,  1.3444e-03,  7.1119e-02,  ...,  6.5841e+00,\n",
      "           -2.0848e+00,  4.5283e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2940e+01, -1.5642e+01, -1.1015e+01,  ..., -1.4093e+01,\n",
      "           -1.3311e+01, -5.1851e-02],\n",
      "          [-1.4525e+01, -9.9441e+00, -8.3553e+00,  ..., -6.8972e+00,\n",
      "           -1.0378e+01,  1.3132e+00],\n",
      "          [-1.5698e+01, -1.2826e+01, -7.0901e+00,  ..., -6.5434e+00,\n",
      "           -1.0493e+01,  3.1280e+00],\n",
      "          ...,\n",
      "          [-5.8713e+00, -3.9480e+00, -1.2045e+00,  ...,  2.0615e-02,\n",
      "           -2.4618e+00,  1.6330e+00],\n",
      "          [-5.5370e+00, -3.6574e+00, -1.1084e+00,  ...,  2.5103e-01,\n",
      "           -1.1314e+00,  1.5163e+00],\n",
      "          [-2.6400e+00, -2.0989e-01,  1.7286e-01,  ...,  2.0073e+00,\n",
      "            1.5542e-01,  1.6442e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.2740e+01, -1.6682e+01, -1.0620e+01,  ..., -1.3618e+01,\n",
      "           -1.2462e+01,  3.4953e-01],\n",
      "          [-1.2689e+01, -9.3911e+00, -7.0802e+00,  ..., -5.6534e+00,\n",
      "           -9.7107e+00,  1.7896e+00],\n",
      "          [-1.5759e+01, -1.2050e+01, -5.7348e+00,  ..., -5.1358e+00,\n",
      "           -1.0498e+01,  3.3995e+00],\n",
      "          ...,\n",
      "          [-4.7163e+00, -4.7643e+00,  6.9618e-01,  ..., -6.5533e-01,\n",
      "           -3.8587e+00,  3.4563e+00],\n",
      "          [-5.6172e+00, -2.6002e+00,  2.8956e+00,  ...,  3.6763e+00,\n",
      "           -1.2002e+00,  3.8178e+00],\n",
      "          [-9.2811e-01,  2.2765e+00,  2.4420e+00,  ...,  6.4555e+00,\n",
      "           -3.4899e-01,  2.8508e+00]]],\n",
      "\n",
      "\n",
      "        [[[-9.3787e+00, -1.3634e+01, -6.8109e+00,  ..., -1.5819e+01,\n",
      "           -1.0685e+01,  4.5337e-01],\n",
      "          [-9.4669e+00, -8.3380e+00, -4.1538e+00,  ..., -6.5179e+00,\n",
      "           -7.2360e+00,  1.1920e+00],\n",
      "          [-1.3144e+01, -1.0671e+01, -2.8428e+00,  ..., -6.0516e+00,\n",
      "           -5.5945e+00,  3.5136e+00],\n",
      "          ...,\n",
      "          [-8.6415e+00, -3.3368e-01,  1.7178e+00,  ...,  1.5158e+00,\n",
      "           -4.8341e+00,  4.4319e+00],\n",
      "          [-1.0199e+01, -2.4859e+00,  1.6877e+00,  ...,  3.2506e+00,\n",
      "           -1.5442e+00,  4.5892e+00],\n",
      "          [-7.2005e+00, -1.9486e-01,  4.0739e+00,  ...,  5.5696e+00,\n",
      "            2.2583e-01,  3.7350e+00]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.randn(1,1,100,100)\n",
    "fixations1 = torch.Tensor(([44,44]))\n",
    "fixations1 = fixations1.view(1,1,2).long()\n",
    "fixations2 = torch.zeros(1,100,100)\n",
    "fixations2[0,44,44] = 1\n",
    "#fixations2[0,23,32] = 1\n",
    "fixations1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-81.2100)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output) - torch.log(output[0,0,44,44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1 = myLoss(output, fixations1)\n",
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.PoissonNLLLoss(log_input=False)\n",
    "loss2 = criterion(output.view(1,100,100), fixations2)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output.view(1,100,100) - torch.log(output.view(1,100,100))*fixations2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output.view(1,100,100)*fixations2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(639734.8750, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4964, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 100, 100])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(tensor_unfl, idx_fl):\n",
    "    \"\"\"\n",
    "    Takes unflattened 2D-tensor and index of the same flattened 2D-tensor and returns the corresponding index\n",
    "    of the unflattened tensor.\n",
    "    \"\"\"\n",
    "    #row_number of unflattened tensor is index of flattened tensor // amount of columns of unflattened tensor\n",
    "    #col_number of unflattened tensor is index of flattened tensor % amount of columns of unflattened tensor\n",
    "    n_cols = tensor_unfl.size()[-1]\n",
    "    row_idx_unfl = idx_fl // n_cols\n",
    "    col_idx_unfl = idx_fl % n_cols\n",
    "    #result = torch.tensor([row_idx_unfl, col_idx_unfl])\n",
    "    #if gpu:\n",
    "    #    if torch.cuda.is_available():\n",
    "    #        result = result.to('cuda')\n",
    "    return (row_idx_unfl, col_idx_unfl)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def accuracy(activations, fixations, fixation_locs, gpu):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy for one image's activations and its corresponding fixation sequence.\n",
    "    \n",
    "    Takes: - activations: tensor of activations of size (1,act_x,act_y) (1 channel)\n",
    "           - fixations: tensor of fixations sequence of size (x,2)\n",
    "    \n",
    "    Returns: - accuracy for this image: 1, if eg 4 fixations, and each fixation location is equal to one of the \n",
    "               the 4 biggest activation values. So eg 0.75, if only 3 fixation locations hit one of the four biggest \n",
    "               activation values and so on\n",
    "            - hits for this image: how many fixations led to one of the x biggest activation values?\n",
    "            - number of fixations this image had in this trial\n",
    "    \"\"\"\n",
    "\n",
    "    ##Accuracy for one image\n",
    "    \n",
    "    #already done in the training/val/eval loop\n",
    "    #drop unnecessary first dimension of activations (there is only one channel)\n",
    "    #activations = activations.reshape(activations.size()[-2], activations.size()[-1])\n",
    "\n",
    "    #how many fixations are there?\n",
    "    #num_fix = 0\n",
    "    #for i,j in fixations:\n",
    "    #            if (i,j) == (-1000,-1000):\n",
    "    #                break\n",
    "    #            num_fix += 1\n",
    "                \n",
    "    #extract the fixated locations\n",
    "    #locations = []\n",
    "    #for i in range(fixations.size()[0]):\n",
    "    #    for j in range(fixations.size()[1]):\n",
    "    #        if fixations[i,j] != 0:\n",
    "    #            locations.append((i,j))\n",
    "    \n",
    "    #extract fixated locations from tensor form\n",
    "    #locations = []\n",
    "    #for i in range(len(fixation_locs)):\n",
    "    #    locations.append(tuple(fixation_locs.tolist()[i]))\n",
    "                \n",
    "    #How many fixations are there?\n",
    "    num_fix = int(torch.sum(fixations).item())\n",
    "    \n",
    "    #select only the first num_fix entries of fixation_locs (rest is (-1000,-1000))\n",
    "    fixation_locs = fixation_locs[0:num_fix]\n",
    "    \n",
    "    #extract fixated locations from expanded tensor\n",
    "    locations = []\n",
    "    fixations_l = fixation_locs.tolist()\n",
    "    #everything coming after the indx num_fix is only (-1000,-1000)\n",
    "    for i in range(num_fix):\n",
    "        locations.append(tuple(fixations_l[i]))\n",
    "    \n",
    "    #flatten activations\n",
    "    activations_f = activations.view(-1)\n",
    "\n",
    "    #find x largest values and their indices in flattened activation-tensor\n",
    "    lar_val, lar_val_idx = torch.topk(activations_f, num_fix)\n",
    "    \n",
    "    idx_unfl = []\n",
    "    for idx_fl in lar_val_idx:\n",
    "        idx_unfl.append(map_idx(activations, idx_fl.item()))\n",
    "\n",
    "    #see if they match with fixations indices\n",
    "    #hits = 0\n",
    "    #does each fixation lead to one of the x biggest activation values?\n",
    "    #for fix in range(num_fix):\n",
    "    #    for idx in idx_unfl:\n",
    "    #        current = torch.all(torch.eq(idx,fixations[fix]))\n",
    "    #        hits += current.item()\n",
    "    \n",
    "    #see if they match with fixations indices\n",
    "    hits = 0\n",
    "    \n",
    "    #print(\"Fixation locations: {}\".format(locations))\n",
    "    #print(\"Indices of biggest activations: {}\".format(idx_unfl))\n",
    "    \n",
    "    #does each fixation lead to one of the x biggest activation values?\n",
    "    biggest_activation_locs = torch.zeros(activations.size())\n",
    "    \n",
    "    for idx in idx_unfl:\n",
    "        biggest_activation_locs[idx] = 1\n",
    "    \n",
    "    if gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            biggest_activation_locs = biggest_activation_locs.to('cuda')\n",
    "    \n",
    "    hits = int(torch.sum(biggest_activation_locs * fixations).item())\n",
    "    \n",
    "    #for fix in locations:\n",
    "    #    for idx in idx_unfl:\n",
    "            #convert the tensor holding the fixation value to \n",
    "            #current = torch.all(torch.eq(idx, fixations[fix].long()))\n",
    "            #hits += current.item()\n",
    "    #        if fix == idx:\n",
    "    #            hits += 1\n",
    "    \n",
    "    #calcualte proportion of hits\n",
    "    acc = hits / num_fix\n",
    "    \n",
    "    return acc, hits, num_fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint_batch_size_128_lr_0.0001.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test loss is: 0.01585181766652806\n",
      "Mean accuracy for test set ist: 0.000543421101674545\n",
      "Hits: 95\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.PoissonNLLLoss(log_input=False)\n",
    "gpu = True\n",
    "\n",
    "#evaluate the model\n",
    "# to track the training loss as the model trains\n",
    "test_losses = []\n",
    "#to track the accuracy \n",
    "acc_per_image = []\n",
    "acc_per_batch = []\n",
    "#track absolute hits\n",
    "hit_list = []\n",
    "#track number of fixations\n",
    "n_fixations = []\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "t = iter(train_loader)\n",
    "for i, example in enumerate(t): #start at index 0\n",
    "            # get the inputs\n",
    "            data = example[\"image\"]\n",
    "            #print(\"input sum: {}\".format(torch.sum(data)))\n",
    "            target = example[\"fixations\"]\n",
    "            target_locs = example[\"fixation_locs\"]\n",
    "            \n",
    "            #push data and targets to gpu\n",
    "            if gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.to('cuda')\n",
    "                    target = target.to('cuda')\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            #drop channel-dimension (is only 1) so that outputs will be of same size as targets (batch_size,100,100)\n",
    "            output = output.view(-1, target.size()[-2], target.size()[-2])\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            # calculate the loss\n",
    "            #loss = myLoss(output, target)\n",
    "            # record training loss\n",
    "            test_losses.append(loss.item())\n",
    "            #accuracy\n",
    "            acc_this_batch = 0\n",
    "            for batch_idx in range(output.size()[0]):\n",
    "                output_subset = output[batch_idx]\n",
    "                target_subset = target[batch_idx]\n",
    "                target_locs_subset = target_locs[batch_idx]\n",
    "                acc_this_image, hits, num_fix = accuracy(output_subset, target_subset, target_locs_subset, gpu)\n",
    "                acc_per_image.append(acc_this_image)\n",
    "                hit_list.append(hits)\n",
    "                n_fixations.append(num_fix)\n",
    "                acc_this_batch += acc_this_image\n",
    "            #divide by batch size\n",
    "            acc_this_batch /= output.size()[0]\n",
    "            acc_per_batch.append(acc_this_batch)\n",
    "                \n",
    "acc_per_image = np.asarray(acc_per_image)\n",
    "print(\"Mean test loss is: {}\".format(np.average(test_losses)))\n",
    "print(\"Mean accuracy for test set ist: {}\".format(np.mean(acc_per_image)))\n",
    "print(\"Hits: {}\".format(sum(hit_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#find indices of these values in unflattend activation-tensor\n",
    "def map_idx(tensor_unfl, idx_fl):\n",
    "    \"\"\"\n",
    "    Takes unflattened 2D-tensor and index of the same flattened 2D-tensor and returns the corresponding index\n",
    "    of the unflattened tensor.\n",
    "    \"\"\"\n",
    "    #row_number of unflattened tensor is index of flattened tensor // amount of columns of unflattened tensor\n",
    "    #col_number of unflattened tensor is index of flattened tensor % amount of columns of unflattened tensor\n",
    "    n_cols = tensor_unfl.size()[-1]\n",
    "    row_idx_unfl = idx_fl // n_cols\n",
    "    col_idx_unfl = idx_fl % n_cols\n",
    "    #result = torch.tensor([row_idx_unfl, col_idx_unfl])\n",
    "    #if gpu:\n",
    "    #    if torch.cuda.is_available():\n",
    "    #        result = result.to('cuda')\n",
    "    return (row_idx_unfl, col_idx_unfl)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def accuracy(activations, fixations, fixation_locs):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy for one image's activations and its corresponding fixation sequence.\n",
    "    \n",
    "    Takes: - activations: tensor of activations of size (1,act_x,act_y) (1 channel)\n",
    "           - fixations: tensor of fixations sequence of size (x,2)\n",
    "    \n",
    "    Returns: - accuracy for this image: 1, if eg 4 fixations, and each fixation location is equal to one of the \n",
    "               the 4 biggest activation values. So eg 0.75, if only 3 fixation locations hit one of the four biggest \n",
    "               activation values and so on\n",
    "            - hits for this image: how many fixations led to one of the x biggest activation values?\n",
    "            - number of fixations this image had in this trial\n",
    "    \"\"\"\n",
    "\n",
    "    ##Accuracy for one image\n",
    "    \n",
    "    #already done in the training/val/eval loop\n",
    "    #drop unnecessary first dimension of activations (there is only one channel)\n",
    "    #activations = activations.reshape(activations.size()[-2], activations.size()[-1])\n",
    "\n",
    "    #how many fixations are there?\n",
    "    #num_fix = 0\n",
    "    #for i,j in fixations:\n",
    "    #            if (i,j) == (-1000,-1000):\n",
    "    #                break\n",
    "    #            num_fix += 1\n",
    "                \n",
    "    #extract the fixated locations\n",
    "    #locations = []\n",
    "    #for i in range(fixations.size()[0]):\n",
    "    #    for j in range(fixations.size()[1]):\n",
    "    #        if fixations[i,j] != 0:\n",
    "    #            locations.append((i,j))\n",
    "    \n",
    "    #extract fixated locations from tensor form\n",
    "    #locations = []\n",
    "    #for i in range(len(fixation_locs)):\n",
    "    #    locations.append(tuple(fixation_locs.tolist()[i]))\n",
    "                \n",
    "    #How many fixations are there?\n",
    "    num_fix = int(torch.sum(fixations).item())\n",
    "    \n",
    "    #extract fixated locations from expanded tensor\n",
    "    locations = []\n",
    "    fixations_l = fixation_locs.tolist()\n",
    "    #everything coming after the indx num_fix is only (-1000,-1000)\n",
    "    for i in range(num_fix):\n",
    "        locations.append(tuple(fixations_l[i]))\n",
    "    \n",
    "    #flatten activations\n",
    "    activations_f = activations.view(-1)\n",
    "\n",
    "    #find x largest values and their indices in flattened activation-tensor\n",
    "    lar_val, lar_val_idx = torch.topk(activations_f, num_fix)\n",
    "    \n",
    "    idx_unfl = []\n",
    "    for idx_fl in lar_val_idx:\n",
    "        idx_unfl.append(map_idx(activations, idx_fl.item()))\n",
    "\n",
    "    #see if they match with fixations indices\n",
    "    #hits = 0\n",
    "    #does each fixation lead to one of the x biggest activation values?\n",
    "    #for fix in range(num_fix):\n",
    "    #    for idx in idx_unfl:\n",
    "    #        current = torch.all(torch.eq(idx,fixations[fix]))\n",
    "    #        hits += current.item()\n",
    "    \n",
    "    #see if they match with fixations indices\n",
    "    hits = 0\n",
    "    #does each fixation lead to one of the x biggest activation values?\n",
    "    for fix in locations:\n",
    "        for idx in idx_unfl:\n",
    "            #convert the tensor holding the fixation value to \n",
    "            #current = torch.all(torch.eq(idx, fixations[fix].long()))\n",
    "            #hits += current.item()\n",
    "            if fix == idx:\n",
    "                hits += 1\n",
    "    \n",
    "    #calcualte proportion of hits\n",
    "    acc = hits / num_fix\n",
    "    \n",
    "    return acc, hits, num_fix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(test_loader, 0):\n",
    "    if i == 0:\n",
    "        output = torch.randn(100,100)\n",
    "        fixations = example[\"fixations\"]\n",
    "        fixation_locs = example[\"fixation_locs\"]\n",
    "        print(fixation_locs[0].size())\n",
    "        acc, hits, num_fix = accuracy(output, fixations[0], fixation_locs[0])\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_locs\n",
    "output[55,45] = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, hits, num_fix = accuracy(output, fixations[0], fixation_locs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55, 45],\n",
       "        [46, 52],\n",
       "        [36, 67],\n",
       "        [31, 72],\n",
       "        [63, 70]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = fixation_locs[0]\n",
    "test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixation_locs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = lambda epoch: epoch // 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(([1,2,3],[4,5,6]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for elem in b:\n",
    "    print(elem.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 100, 100])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(128,1,100,100)\n",
    "b = torch.randn(100,100)\n",
    "(a * b).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([2])\n",
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(a-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
